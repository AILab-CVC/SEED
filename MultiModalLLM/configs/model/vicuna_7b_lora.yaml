_target_: src.model.peft_model.get_peft_model_with_resize_embedding
model:
  _target_: src.model.llama_xformer.LlamaForCausalLM.from_pretrained
  pretrained_model_name_or_path: pretrained/vicuna-7b-v1.1
peft_config:
  _target_: peft.LoraConfig
  _convert_: object
  r: 16
  lora_alpha: 32
  modules_to_save:
    - embed_tokens
    - lm_head
    - input_layernorm
    - post_attention_layernorm
    - norm
  target_modules: 
    - q_proj 
    - v_proj 
    - k_proj 
    - o_proj 
    - gate_proj 
    - down_proj 
    - up_proj
  task_type: CAUSAL_LM
  lora_dropout: 0.05